# Ollama Setup Guide

## üöÄ Quick Start with Free Local Models

This system uses **Ollama** for free, local AI models - no API keys or cloud costs!

### 1. Install Ollama

**Windows:**
Download from https://ollama.ai/download

**Mac:**
```bash
brew install ollama
```

**Linux:**
```bash
curl https://ollama.ai/install.sh | sh
```

### 2. Start Ollama Service

```bash
ollama serve
```

Keep this running in a terminal.

### 3. Pull Required Models

**Main Model (for reasoning):**
```bash
# Recommended: Llama 3.2 (7B - good balance)
ollama pull llama3.2

# Alternatives:
ollama pull mistral       # Fast and efficient
ollama pull llama2        # Reliable, older
ollama pull phi           # Lightweight (3B)
ollama pull codellama     # Good for code
```

**Embedding Model (for memory/search):**
```bash
ollama pull nomic-embed-text
```

### 4. Test Models

```bash
# Test main model
ollama run llama3.2 "Hello, how are you?"

# Test embedding model
ollama run nomic-embed-text "Test embedding"
```

### 5. Configure .env

```env
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
```

### 6. Run the System

```bash
python main.py
```

## üìä Model Comparison

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| **llama3.2** | 7B | Medium | High | General purpose ‚≠ê |
| **mistral** | 7B | Fast | High | Fast responses |
| **phi** | 3B | Very Fast | Medium | Low-end hardware |
| **llama2** | 7B | Medium | Good | Stable, tested |
| **codellama** | 7B | Medium | High | Code-heavy tasks |

## üéØ Model Recommendations

### For Low-End PC (8GB RAM)
```bash
ollama pull phi
```
```env
OLLAMA_MODEL=phi
```

### For Mid-Range PC (16GB RAM)
```bash
ollama pull llama3.2
```
```env
OLLAMA_MODEL=llama3.2
```

### For High-End PC (32GB+ RAM)
```bash
ollama pull llama2:13b
```
```env
OLLAMA_MODEL=llama2:13b
```

## üîß Troubleshooting

### Ollama Not Running
```bash
# Check if running
curl http://localhost:11434/api/tags

# Start Ollama
ollama serve
```

### Model Not Found
```bash
# List installed models
ollama list

# Pull missing model
ollama pull llama3.2
```

### Out of Memory
- Use smaller model: `phi` or `mistral`
- Close other applications
- Reduce context window in code

### Slow Responses
- Use faster model: `mistral` or `phi`
- Enable GPU acceleration (automatic if available)
- Reduce max_tokens in prompts

## üí∞ Cost Comparison

### OpenAI (Paid)
- GPT-4: $0.03 per 1K input tokens
- GPT-3.5: $0.0015 per 1K tokens
- **Monthly cost for active system: $50-200+**

### Ollama (Free)
- All models: **$0** forever
- Only cost: Your electricity (~$2-5/month)
- No usage limits
- No API keys needed
- Data stays local (privacy!)

## üöÄ Advanced: GPU Acceleration

Ollama automatically uses GPU if available (NVIDIA, AMD, or Apple Silicon).

### Check GPU Usage
```bash
# NVIDIA
nvidia-smi

# AMD
rocm-smi

# Mac
# GPU is used automatically
```

### Performance with GPU
- 10-50x faster than CPU
- Can run larger models (13B, 70B)
- Better quality responses

## üìù Switching Models

To switch models anytime:

```bash
# Pull new model
ollama pull mistral

# Update .env
OLLAMA_MODEL=mistral

# Restart system
python main.py
```

## üéì Tips

1. **Start with llama3.2** - best balance of speed and quality
2. **Keep Ollama running** - `ollama serve` in background
3. **Pre-pull models** - download before first use
4. **Monitor RAM** - close other apps if slow
5. **Try different models** - each has strengths

## üÜö Ollama vs OpenAI

| Feature | Ollama | OpenAI |
|---------|---------|--------|
| Cost | FREE | $50-200/mo |
| Privacy | Local | Cloud |
| Speed | Good (GPU) | Fast |
| Quality | Very Good | Excellent |
| Setup | 5 minutes | Instant |
| Internet | Not needed | Required |
| Limits | None | Rate limits |

## ‚úÖ You're Ready!

```bash
# 1. Start Ollama
ollama serve

# 2. Start your system
python main.py

# 3. Test it
# Send WhatsApp message or use test endpoint
```

Congratulations! You're running a completely free, local AI system! üéâ
